{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries first:"
      ],
      "metadata": {
        "id": "ygXJxx6oD0mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install openai==0.27.10 faiss-gpu tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ru3LGE3D2SR",
        "outputId": "338906fe-184a-422c-d92b-5c5990bdd6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.27.10 in /usr/local/lib/python3.10/dist-packages (0.27.10)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.10) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.10) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.10) (3.10.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.10) (4.12.2)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Setup OpenAI API Key"
      ],
      "metadata": {
        "id": "QOL-bTVLDisy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "6f5tWzbYG4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "with open('secrets/config.json', 'r') as config:\n",
        "    data = config.read()\n",
        "\n",
        "creds = json.loads(data)\n",
        "\n",
        "# OpenAI API key is set here\n",
        "openai.api_key = creds[\"AZURE_OPENAI_KEY\"]"
      ],
      "metadata": {
        "id": "mlQwtMEqD6Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Read the Large Text Document"
      ],
      "metadata": {
        "id": "EHfEusORNxHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/large-text-file-to-query.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "BdB4BE_XNz3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Split the Text into Chunks"
      ],
      "metadata": {
        "id": "QtOzc16ID_Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=500, overlap=50):   # Size value is in tokens\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding('cl100k_base')  # Use the appropriate encoding\n",
        "\n",
        "    # Tokenize the entire text\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    chunks = []\n",
        "    text_length = len(tokens)\n",
        "    start = 0\n",
        "\n",
        "    while start < text_length:\n",
        "        end = start + chunk_size\n",
        "        chunk_tokens = tokens[start:end]\n",
        "        chunk = tokenizer.decode(chunk_tokens)\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap  # Move the window forward by chunk_size minus overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Use the updated function\n",
        "chunks = split_text_into_chunks(text)\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "NEqL7idPEAHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62868f54-564d-43cc-ea31-7e625d80145f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 1125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Generate Embeddings and Build the Vector Index"
      ],
      "metadata": {
        "id": "cTQTliWWKfBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=model\n",
        "    )\n",
        "    embedding = response['data'][0]['embedding']\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "hO-yZOdHEGFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for each chunk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Generate embeddings with a progress bar\n",
        "embeddings = []\n",
        "chunk_texts = []\n",
        "\n",
        "for idx, chunk in enumerate(tqdm(chunks, desc=\"Generating embeddings\")):\n",
        "    if chunk.strip():\n",
        "        embedding = get_embedding(chunk)\n",
        "        embeddings.append(embedding)\n",
        "        chunk_texts.append(chunk)\n"
      ],
      "metadata": {
        "id": "okJrUmjYOyie",
        "outputId": "ff9d6a45-f651-4890-9417-ec423b70f627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 1125/1125 [06:09<00:00,  3.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# Convert embeddings to a numpy array and build the FAISS index\n",
        "embedding_matrix = np.array(embeddings).astype('float32')\n",
        "embedding_size = embedding_matrix.shape[1]\n",
        "\n",
        "# Build the FAISS index\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "index.add(embedding_matrix)\n",
        "print(f\"FAISS index has {index.ntotal} vectors.\")\n"
      ],
      "metadata": {
        "id": "OtaM7x34O1EW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99144627-bdd6-43b1-e6b2-33817455044c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index has 1125 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Save the Index and Chunks (Optional)"
      ],
      "metadata": {
        "id": "2u1ZocIbEMUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the FAISS index\n",
        "faiss.write_index(index, \"faiss_index.index\")\n",
        "\n",
        "# Save the chunk texts\n",
        "with open('chunk_texts.pkl', 'wb') as f:\n",
        "    pickle.dump(chunk_texts, f)\n"
      ],
      "metadata": {
        "id": "IxxVszn0EM-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Load the Index and Chunks (Optional)"
      ],
      "metadata": {
        "id": "5ooDOZKEEVdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FAISS index\n",
        "index = faiss.read_index(\"faiss_index.index\")\n",
        "\n",
        "# Load the chunk texts\n",
        "with open('chunk_texts.pkl', 'rb') as f:\n",
        "    chunk_texts = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "F9bv5QNEEWNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Define the Search and Answer Generation Functions"
      ],
      "metadata": {
        "id": "kDTEDaoyDwWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, top_k_hits=5):\n",
        "    query_embedding = get_embedding(query)\n",
        "    query_vector = np.array([query_embedding]).astype('float32')\n",
        "    distances, indices = index.search(query_vector, top_k_hits)\n",
        "    return indices[0], distances[0]"
      ],
      "metadata": {
        "id": "f6INQNmURrfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, top_k_hits=5):\n",
        "    indices, distances = search(query, top_k_hits)\n",
        "    relevant_chunks = [chunk_texts[i] for i in indices]\n",
        "\n",
        "    # Combine the relevant chunks\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = f\"Answer the following question using the context provided.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    # Use GPT-4 to generate the answer\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    answer = response['choices'][0]['message']['content']\n",
        "    return answer"
      ],
      "metadata": {
        "id": "yvUkgZeCRtsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Query the Document"
      ],
      "metadata": {
        "id": "pYTDa04aRw9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input your query\n",
        "query = \"Is there any communication suggesting that Sean expected to repay the note (bridge loan) to DJ using the CPUcoin cryptocurrency?\"\n",
        "\n",
        "# Generate the answer\n",
        "answer = generate_answer(query)\n",
        "print(\"Response:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osb6k8PTRy-R",
        "outputId": "9e7d517e-6d63-4a44-cb41-655dd5b022ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "No, there is no specific communication suggesting that Sean expected to repay the bridge loan to DJ using the CPUcoin cryptocurrency. Sean refers to the bridge loan as a convertible note that converts into equity but does not explicitly state that CPUcoin would be used for repayment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input your query\n",
        "query = \"Is there any communication about Keith?\"\n",
        "\n",
        "# Generate the answer\n",
        "answer = generate_answer(query, 5)\n",
        "print(\"Response:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DudofSQxZgLq",
        "outputId": "1c8f8703-f516-459f-dccd-f557e45edb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "Yes, there is communication about Keith. DJ has been regularly updating his distribution list on Keith's health condition. In the most recent update, he explained that Keith was released from UCSF Sunday evening and was able to sleep well in his own bed. He also mentioned having to take Keith to his twice weekly clinic for anti-rejection medications, dealing with some bureaucratic issues, and assisting Keith with hygiene and mobility at home. Previously, DJ reported that Keith's recovery has been swift and he has been able to spend time out of bed and receive visitors.\n"
          ]
        }
      ]
    }
  ]
}