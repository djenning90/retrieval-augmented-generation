{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries first:"
      ],
      "metadata": {
        "id": "ygXJxx6oD0mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install openai==0.27.10 faiss-cpu tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ru3LGE3D2SR",
        "outputId": "4f36c7b1-5ac5-4b16-b7a6-9e5ebf81f359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.27.10 in /usr/local/lib/python3.10/dist-packages (0.27.10)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.10) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.10) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.10) (3.10.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.10) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.10) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.10) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Setup OpenAI API Key"
      ],
      "metadata": {
        "id": "QOL-bTVLDisy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import openai\n",
        "import faiss\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import pickle"
      ],
      "metadata": {
        "id": "6f5tWzbYG4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "with open('config.json', 'r') as config:\n",
        "    data = config.read()\n",
        "\n",
        "creds = json.loads(data)\n",
        "\n",
        "# OpenAI API key is set here\n",
        "openai.api_key = creds[\"AZURE_OPENAI_KEY\"]"
      ],
      "metadata": {
        "id": "mlQwtMEqD6Ha"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Read the Large Text Document"
      ],
      "metadata": {
        "id": "EHfEusORNxHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/DJ and Sean Barger skype transcript', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "BdB4BE_XNz3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Split the Text into Chunks"
      ],
      "metadata": {
        "id": "QtOzc16ID_Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_chunks(text, max_tokens=500):\n",
        "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "    sentences = text.split('. ')\n",
        "    chunks = []\n",
        "    current_chunk = ''\n",
        "    current_tokens = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence += '. '\n",
        "        tokens = tokenizer.encode(sentence)\n",
        "        token_count = len(tokens)\n",
        "        if current_tokens + token_count > max_tokens:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "            current_tokens = token_count\n",
        "        else:\n",
        "            current_chunk += sentence\n",
        "            current_tokens += token_count\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text_into_chunks(text)\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "NEqL7idPEAHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ace3ed3-74d9-4d94-ad2d-cd22b218573d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 1117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Generate Embeddings and Build the Vector Index"
      ],
      "metadata": {
        "id": "cTQTliWWKfBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=model\n",
        "    )\n",
        "    embedding = response['data'][0]['embedding']\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "hO-yZOdHEGFh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for each chunk\n",
        "embeddings = []\n",
        "chunk_texts = []\n",
        "\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    embedding = get_embedding(chunk)\n",
        "    embeddings.append(embedding)\n",
        "    chunk_texts.append(chunk)\n",
        "    if (idx + 1) % 10 == 0 or (idx + 1) == len(chunks):\n",
        "        print(f\"Processed chunk {idx + 1}/{len(chunks)}\")\n"
      ],
      "metadata": {
        "id": "okJrUmjYOyie",
        "outputId": "2019ff6a-a035-4cc9-af4b-29f571410fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 10/1117\n",
            "Processed chunk 20/1117\n",
            "Processed chunk 30/1117\n",
            "Processed chunk 40/1117\n",
            "Processed chunk 50/1117\n",
            "Processed chunk 60/1117\n",
            "Processed chunk 70/1117\n",
            "Processed chunk 80/1117\n",
            "Processed chunk 90/1117\n",
            "Processed chunk 100/1117\n",
            "Processed chunk 110/1117\n",
            "Processed chunk 120/1117\n",
            "Processed chunk 130/1117\n",
            "Processed chunk 140/1117\n",
            "Processed chunk 150/1117\n",
            "Processed chunk 160/1117\n",
            "Processed chunk 170/1117\n",
            "Processed chunk 180/1117\n",
            "Processed chunk 190/1117\n",
            "Processed chunk 200/1117\n",
            "Processed chunk 210/1117\n",
            "Processed chunk 220/1117\n",
            "Processed chunk 230/1117\n",
            "Processed chunk 240/1117\n",
            "Processed chunk 250/1117\n",
            "Processed chunk 260/1117\n",
            "Processed chunk 270/1117\n",
            "Processed chunk 280/1117\n",
            "Processed chunk 290/1117\n",
            "Processed chunk 300/1117\n",
            "Processed chunk 310/1117\n",
            "Processed chunk 320/1117\n",
            "Processed chunk 330/1117\n",
            "Processed chunk 340/1117\n",
            "Processed chunk 350/1117\n",
            "Processed chunk 360/1117\n",
            "Processed chunk 370/1117\n",
            "Processed chunk 380/1117\n",
            "Processed chunk 390/1117\n",
            "Processed chunk 400/1117\n",
            "Processed chunk 410/1117\n",
            "Processed chunk 420/1117\n",
            "Processed chunk 430/1117\n",
            "Processed chunk 440/1117\n",
            "Processed chunk 450/1117\n",
            "Processed chunk 460/1117\n",
            "Processed chunk 470/1117\n",
            "Processed chunk 480/1117\n",
            "Processed chunk 490/1117\n",
            "Processed chunk 500/1117\n",
            "Processed chunk 510/1117\n",
            "Processed chunk 520/1117\n",
            "Processed chunk 530/1117\n",
            "Processed chunk 540/1117\n",
            "Processed chunk 550/1117\n",
            "Processed chunk 560/1117\n",
            "Processed chunk 570/1117\n",
            "Processed chunk 580/1117\n",
            "Processed chunk 590/1117\n",
            "Processed chunk 600/1117\n",
            "Processed chunk 610/1117\n",
            "Processed chunk 620/1117\n",
            "Processed chunk 630/1117\n",
            "Processed chunk 640/1117\n",
            "Processed chunk 650/1117\n",
            "Processed chunk 660/1117\n",
            "Processed chunk 670/1117\n",
            "Processed chunk 680/1117\n",
            "Processed chunk 690/1117\n",
            "Processed chunk 700/1117\n",
            "Processed chunk 710/1117\n",
            "Processed chunk 720/1117\n",
            "Processed chunk 730/1117\n",
            "Processed chunk 740/1117\n",
            "Processed chunk 750/1117\n",
            "Processed chunk 760/1117\n",
            "Processed chunk 770/1117\n",
            "Processed chunk 780/1117\n",
            "Processed chunk 790/1117\n",
            "Processed chunk 800/1117\n",
            "Processed chunk 810/1117\n",
            "Processed chunk 820/1117\n",
            "Processed chunk 830/1117\n",
            "Processed chunk 840/1117\n",
            "Processed chunk 850/1117\n",
            "Processed chunk 860/1117\n",
            "Processed chunk 870/1117\n",
            "Processed chunk 880/1117\n",
            "Processed chunk 890/1117\n",
            "Processed chunk 900/1117\n",
            "Processed chunk 910/1117\n",
            "Processed chunk 920/1117\n",
            "Processed chunk 930/1117\n",
            "Processed chunk 940/1117\n",
            "Processed chunk 950/1117\n",
            "Processed chunk 960/1117\n",
            "Processed chunk 970/1117\n",
            "Processed chunk 980/1117\n",
            "Processed chunk 990/1117\n",
            "Processed chunk 1000/1117\n",
            "Processed chunk 1010/1117\n",
            "Processed chunk 1020/1117\n",
            "Processed chunk 1030/1117\n",
            "Processed chunk 1040/1117\n",
            "Processed chunk 1050/1117\n",
            "Processed chunk 1060/1117\n",
            "Processed chunk 1070/1117\n",
            "Processed chunk 1080/1117\n",
            "Processed chunk 1090/1117\n",
            "Processed chunk 1100/1117\n",
            "Processed chunk 1110/1117\n",
            "Processed chunk 1117/1117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert embeddings to a numpy array and build the FAISS index\n",
        "embedding_matrix = np.array(embeddings).astype('float32')\n",
        "embedding_size = embedding_matrix.shape[1]\n",
        "\n",
        "# Build the FAISS index\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "index.add(embedding_matrix)\n",
        "print(f\"FAISS index has {index.ntotal} vectors.\")\n"
      ],
      "metadata": {
        "id": "OtaM7x34O1EW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bda514-eab5-4d7f-87e7-d5e01ca853db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index has 1117 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Save the Index and Chunks (Optional)"
      ],
      "metadata": {
        "id": "2u1ZocIbEMUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the FAISS index\n",
        "faiss.write_index(index, \"faiss_index.index\")\n",
        "\n",
        "# Save the chunk texts\n",
        "with open('chunk_texts.pkl', 'wb') as f:\n",
        "    pickle.dump(chunk_texts, f)\n"
      ],
      "metadata": {
        "id": "IxxVszn0EM-m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Load the Index and Chunks (Optional)"
      ],
      "metadata": {
        "id": "5ooDOZKEEVdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FAISS index\n",
        "index = faiss.read_index(\"faiss_index.index\")\n",
        "\n",
        "# Load the chunk texts\n",
        "with open('chunk_texts.pkl', 'rb') as f:\n",
        "    chunk_texts = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "F9bv5QNEEWNG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Define the Search and Answer Generation Functions"
      ],
      "metadata": {
        "id": "kDTEDaoyDwWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, k=5):\n",
        "    query_embedding = get_embedding(query)\n",
        "    query_vector = np.array([query_embedding]).astype('float32')\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "    return indices[0], distances[0]"
      ],
      "metadata": {
        "id": "f6INQNmURrfS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, k=5):\n",
        "    indices, distances = search(query, k)\n",
        "    relevant_chunks = [chunk_texts[i] for i in indices]\n",
        "\n",
        "    # Combine the relevant chunks\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = f\"Answer the following question using the context provided.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    # Use GPT-4 to generate the answer\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    answer = response['choices'][0]['message']['content']\n",
        "    return answer"
      ],
      "metadata": {
        "id": "yvUkgZeCRtsd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Query the Document"
      ],
      "metadata": {
        "id": "pYTDa04aRw9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input your query\n",
        "query = \"Is there any communication suggesting that Sean expected to repay the note (bridge loan) to DJ using the CPUcoin cryptocurrency?\"\n",
        "\n",
        "# Generate the answer\n",
        "answer = generate_answer(query)\n",
        "print(\"Response:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osb6k8PTRy-R",
        "outputId": "4820b19b-a61e-425e-99fd-3902054b1a9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "Yes, there is communication that suggests that Sean Barger expected to repay the bridge loan to DJ. In his message on 01/03/2020 19:18:25, Sean said that he would work on the term sheet that pays down the convertible note with 10% of funds raised at CPUcoin as and when the funds come in, in return for all tokens in DJ's possession.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input your query\n",
        "query = \"Is there any communication about Keith?\"\n",
        "\n",
        "# Generate the answer\n",
        "answer = generate_answer(query)\n",
        "print(\"Response:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DudofSQxZgLq",
        "outputId": "572655fc-b748-4916-d562-21df02cf9dc5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "Yes, there has been extensive communication about Keith. The updates mention how he's recovering, expressing that Keith's joyful personality has won over the staff, and that he was expected to move from the ICU back to the 9th floor ward. He had regained enough energy to accept visitors and take phone calls. Later updates, however, highlighted that Keith was to be readmitted to investigate high bilirubin levels.\n"
          ]
        }
      ]
    }
  ]
}